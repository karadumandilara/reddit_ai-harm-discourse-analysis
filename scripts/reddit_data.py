# -*- coding: utf-8 -*-
"""reddit-data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RfaeOMVRCcyM9IaqY1bTvmGwIgw3j0rN
"""

# Install libraries
!pip install gensim pandas numpy matplotlib seaborn wordcloud pyLDAvis scikit-learn
!pip install praw
# Import everything
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# NLP libraries
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Topic modeling
import gensim
from gensim import corpora
from gensim.models import LdaModel, CoherenceModel

# Visualization
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis

# Download NLTK data
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('omw-1.4')

print(" All libraries loaded!")

import warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)

EXTENDED_HARM_QUERIES = {


    "biometric_harms": [
        "facial recognition wrongful arrest",
        "facial recognition misidentify",
        "facial recognition police mistake",
        "clearview ai privacy",
        "clearview ai lawsuit",
        "facial recognition ban",
        "biometric surveillance",
        "emotion recognition discrimination",
        "facial recognition false match",
        "voice recognition false rejection",
        "gait recognition bias"
    ],

    "algorithmic_discrimination": [
        "ai hiring bias",
        "algorithm discrimination",
        "algorithm racist",
        "apple card gender bias",
        "compas algorithm",
        "compas algorithm bias",
        "predictive policing bias",
        "ai healthcare denial",
        "algorithm credit discrimination",
        "hiring algorithm discrimination",
        "algorithm sentencing bias",
        "ai financial fraud",
        "high frequency trading manipulation",
        "robo advisor investment loss",
        "ai mortgage denial bias",
        "amazon worker surveillance firing"

    ],

    "generative_ai_harms": [
        "deepfake non-consensual",
        "deepfake porn",
        "ai deepfake taylor swift",
        "ai voice cloning scam",
        "ai art theft",
        "midjourney lawsuit",
        "chatgpt plagiarism",
        "ai replacing artists",
        "deepfake revenge porn",
        "deepfake harassment",
        "ai election deepfake",
        "large language model halluncination harm"
    ],

    "platform_algorithm_harms": [
        "instagram algorithm mental health",
        "youtube algorithm radicalization",
        "tiktok algorithm harmful",
        "facebook algorithm misinformation",
        "social media algorithm teen",
        "content moderation error"
    ],

    "automated_system_harms": [
        "automated benefits denied",
        "proctorio false positive",
        "tenant screening algorithm",
        "automated welfare discrimination",
        "algorithm unemployment denied",
        "self-driving car accident death",
        "autonomous vehicle crash victim",
        "drone surveillance incident harm",

    ],

    "environmental_and_resource_harms": [
        "ai large model water consumption",
        "chatgpt energy use",
        "data center carbon footprint",
        "ai training heat waste",
        "cryptocurrency energy scandal"

    ],


    "legal_consequences": [
        "facial recognition lawsuit",
        "algorithm discrimination lawsuit",
        "ai lawsuit",
        "deepfake lawsuit",
        "facial recognition settlement",
        "algorithm class action",
        "sue facial recognition",
        "sued for ai bias",
        "ai patent infringement lawsuit",
        "class action against generative AI",
        "GDPR fine algorithm"

    ],

    "personal_impact": [
        "facial recognition ruined life",
        "algorithm denied job",
        "algorithm denied loan",
        "algorithm denied benefits",
        "deepfake destroyed reputation",
        "ai cost me job",
        "cant get job algorithm",
        "blacklisted by algorithm",
        "algorithm denied housing",
        "denied medical care AI",
        "deepfake identity theft"

    ],

    "systemic_impact": [
        "facial recognition mass surveillance",
        "algorithm systemic discrimination",
        "ai inequality",
        "algorithm perpetuates racism",
        "deepfake democracy threat",
        "ai misinformation social cohesion",
        "concentration of power in big tech",
        "automation job market collapse"
    ],

    "psychological_and_trust_erosion": [
        "ai system PTSD",
        "algorithm anxiety",
        "loss of trust in institutions due to AI",
        "social isolation due to platform algorithm"
    ],

    "accountability_avoidance": [
        "ai company denies responsibility",
        "algorithm failure cover up",
        "lack of recourse for algorithm victim",
        "ai audit secrecy"
    ],



    "first_person_harm": [
        "i was arrested facial recognition",
        "algorithm denied me",
        "ai discriminated against me",
        "deepfake of me",
        "facial recognition got me arrested",
        "algorithm rejected my application",
        "i lost my art to ai",
        "ai tried to scam me",
        "i was shadowbanned by algorithm"
    ],

    "victim_stories": [
        "wrongly accused facial recognition",
        "falsely arrested algorithm",
        "mistakenly identified ai",
        "victim facial recognition",
        "harmed by algorithm",
        "deepfake victim",
        "ai surveillance victim",
        "denied housing algorithm victim",
        "my identity was stolen by deepfake"
    ],

    "impact_testimony": [
        "facial recognition changed my life",
        "algorithm ruined everything",
        "cant trust ai anymore",
        "ai made me paranoid",
        "ai monitoring ruined my marriage",
        "life after deepfake"
    ],

    "empowerment_and_advocacy": [
    "how i fought the algorithm",
    "warn others about ai bias",
    "seeking help for deepfake",
    "my appeal against the algorithm"
    ],



    "moral_outrage": [
        "facial recognition outrageous",
        "algorithm unacceptable",
        "deepfake horrifying",
        "ai gone too far",
        "algorithm violation human rights",
        "facial recognition dystopia",
        "algorithm absolutely wrong",
        "scandalous ai",
        "betrayed by ai",
        "ai future uncertain",
        "privacy anxiety",
        "fear of AI replacing me"
    ],

    "concern_worry": [
        "facial recognition dangerous",
        "algorithm concerning",
        "worried about ai",
        "afraid of facial recognition",
        "algorithm scary",
        "deepfake threat",
        "ai terrifying"
    ],

    "calls_for_action": [
        "ban facial recognition",
        "regulate algorithm",
        "stop ai discrimination",
        "need facial recognition regulation",
        "algorithm accountability",
        "sue for ai discrimination",
        "ai developer code of conduct",
        "revoke facial recognition licence",
        "audit algorithm"
    ],

    "resignation_acceptance": [
        "facial recognition inevitable",
        "algorithm everywhere now",
        "cant stop ai",
        "get used to surveillance",
        "algorithm normalized",
        "accept facial recognition",
        "ai just reality now"
    ],

    "demanding_transparency": [
    "ai system transparency needed",
    "explain the algorithm decision",
    "right to explanation AI",
    "algorithm black box problem"
    ],


    "technical_failures": [
        "facial recognition error",
        "algorithm false positive",
        "ai mistake",
        "facial recognition inaccurate",
        "algorithm wrong",
        "deepfake undetectable",
        "llm hallunication mechanism",
        "model drift failure",
        "overreliance on ai prediction"
    ],

    "bias_mechanisms": [
        "facial recognition biased training data",
        "algorithm perpetuates bias",
        "ai learned discrimination",
        "biased dataset facial recognition",
        "algorithm encodes racism",
        "bias in synthetic data",
        "lack of inclusive testing",
        "historical data perpetuates inequality"
    ],

    "system_design_flaws": [
        "facial recognition no accountability",
        "algorithm black box",
        "ai unexplainable decision",
        "facial recognition no oversight",
        "lack of human in the loop",
        "ai system unpatchable vulnerability",
        "complexity makes it dangerous"
    ],

    "incentive_and_profit_mechanisms": [
      "profit over ethics AI",
      "surveillance capitalism model",
      "engagement optimization harm",
      "data hoarding danger"
      ],


    "frequency_routine": [
        "another facial recognition arrest",
        "yet another algorithm bias",
        "one more deepfake",
        "facial recognition mistake again",
        "algorithm discrimination common",
        "deepfake everywhere now",
        "another ai error",
        "another deepfake video goes viral",
        "expected an AI mistake",
        "daily algorithm failure"
    ],

    "acceptance_language": [
        "facial recognition standard now",
        "algorithm part of life",
        "ai just how it is",
        "facial recognition normal",
        "algorithm expected",
        "getting used to surveillance",
        "ai everywhere accept it",
        "cannot go back before ai",
        "it's the price of convenience",
        "ai is inevitable anyway"
    ],

    "comparative_harms": [
        "worse than facial recognition",
        "not as bad as algorithm",
        "better than human discrimination",
        "facial recognition lesser evil",
        "algorithm improves over time",
        "better than human decision",
        "ai is less corrupt than",
        "ai is just a tool"
    ],
    "personal_adaptation_and_mitigation": [
        "how to avoid facial recognition",
        "changing behavior due to AI",
        "just assume you are watched",
        "using VPN because of AI"
    ],

    "ineffective_fixes": [
        "facial recognition audit failed",
        "algorithm fairness doesnt work",
        "ai bias mitigation ineffective",
        "facial recognition oversight weak",
        "algorithm regulation toothless",
        "no legal power over algorithm",
        "band-aid solution ai",
        "ethics board ignored"
    ],

    "empty_promises": [
        "facial recognition company promises",
        "algorithm transparency claim",
        "ai ethics washing",
        "facial recognition reform insufficient",
        "algorithm accountability theater",
        "ai resposibility claim",
        "ethical ai marketing scam",
        "self regulation failure"
    ],

    "blame_and_suppression_tactics": [
        "algorithm victim blamed",
        "silence ai critics",
        "ai system gaslighting",
        "retaliation for reporting bias"
    ],


    "casual_mentions": [
        "facial recognition privacy concerns aside",
        "algorithm bias issues notwithstanding",
        "despite discrimination concerns",
        "ignoring privacy issues",
        "putting aside ethical concerns",
        "deepfake risk aside",
        "forget about the bias for a second",
        "putting aside accuracy problems"
    ],

    "justified_harms": [
        "facial recognition worth privacy trade",
        "algorithm bias acceptable cost",
        "efficiency outweighs discrimination",
        "security justifies surveillance",
        "convenience over privacy",
        "cost saving justifies algorithm",
        "better resource allocation with ai",
        "speed outweights inaccuracy"
    ],
    "inevitability_and_technological_determinism": [
        "ai is the only way forward",
        "cant avoid the algorithm",
        "must use facial recognition",
        "AI is just progress"
    ]
}

# Count total queries
total_queries = sum(len(queries) for layer in EXTENDED_HARM_QUERIES.values()
                    for queries in (layer if isinstance(layer, list) else layer.values()))
print(f"Total queries: {total_queries}")

import praw
import pandas as pd
from datetime import datetime
import time
from collections import defaultdict

# Initialize Reddit API
reddit = praw.Reddit(
    client_id="xV9I4pLgS9Jbr3SGn_TIAQ",
    client_secret="v4bxPLUAu5v552XG5jFCnBS-CGrnyQ",
    user_agent="AI Harm Research v2.0 by /u/senbenithaka"
)


# Time periods
TIME_PERIODS = {
    '2020 - 2021': {
        'start': datetime(2020, 1, 1).timestamp(),
        'end': datetime(2021, 12, 31).timestamp(),
        'label': '2020 - 2021'
    },
    '2022-2023': {
        'start': datetime(2022, 1, 1).timestamp(),
        'end': datetime(2023, 12, 31).timestamp(),
        'label': '2022-2023'
    },
    '2024-2025': {
        'start': datetime(2024, 1, 1).timestamp(),
        'end': datetime(2025, 11, 3).timestamp(),  # Up to today
        'label': '2024-2025'
    }
}

# Subreddits to search
SUBREDDITS = ['technology', 'artificial', 'privacy', 'Futurology', 'news']

# Flatten all queries into single list with metadata
ALL_QUERIES = []
for layer_name, layer_content in EXTENDED_HARM_QUERIES.items():
    if isinstance(layer_content, dict):
        # Layer has subcategories
        for subcategory, queries in layer_content.items():
            for query in queries:
                ALL_QUERIES.append({
                    'query': query,
                    'layer': layer_name,
                    'subcategory': subcategory
                })
    else:
        # Layer is flat list
        for query in layer_content:
            ALL_QUERIES.append({
                'query': query,
                'layer': layer_name,
                'subcategory': layer_name
            })

print(f"\n Total queries to execute: {len(ALL_QUERIES)}")
print(f" Subreddits: {', '.join(SUBREDDITS)}")
print(f" Time periods: {len(TIME_PERIODS)}")

# ============================================================================
# DATA COLLECTION FUNCTION - WITH POSTS + COMMENTS
# ============================================================================

def collect_reddit_data(queries_list, subreddits, time_periods,
                        posts_per_query=20, comments_per_post=10,
                        rate_limit_delay=2):
    """
    Collect Reddit posts AND comments using extended harm queries

    Parameters:
    - queries_list: List of query dictionaries with metadata
    - subreddits: List of subreddit names
    - time_periods: Dictionary of time period configurations
    - posts_per_query: Number of posts to collect per query (default: 20)
    - comments_per_post: Number of top comments per post (default: 5)
    - rate_limit_delay: Seconds to wait between queries (default: 2)

    Returns:
    - DataFrame with collected posts AND comments
    """

    collected_data = []
    stats = defaultdict(lambda: defaultdict(int))

    print("\n" + "="*80)
    print("STARTING DATA COLLECTION (POSTS + COMMENTS)")
    print("="*80)

    total_operations = len(queries_list) * len(subreddits) * len(time_periods)
    current_operation = 0

    for query_info in queries_list:
        query = query_info['query']
        layer = query_info['layer']
        subcategory = query_info['subcategory']

        print(f"\n{'='*80}")
        print(f"Query: '{query}'")
        print(f"Layer: {layer} | Subcategory: {subcategory}")
        print(f"{'='*80}")

        for period_name, period_info in time_periods.items():
            period_start = period_info['start']
            period_end = period_info['end']

            print(f"\n  ðŸ“… Period: {period_name}")

            period_posts = 0
            period_comments = 0

            for subreddit_name in subreddits:
                current_operation += 1
                progress = (current_operation / total_operations) * 100

                print(f"    [{progress:5.1f}%] r/{subreddit_name}...", end=" ")

                try:
                    subreddit = reddit.subreddit(subreddit_name)

                    # Search with this query
                    search_results = subreddit.search(
                        query,
                        sort='relevance',
                        time_filter='all',
                        limit=posts_per_query
                    )

                    posts_count = 0
                    comments_count = 0

                    for post in search_results:
                        # Check if post is in this time period
                        if period_start <= post.created_utc <= period_end:
                            # ==========================================
                            # COLLECT POST DATA
                            # ==========================================
                            post_data = {
                                'post_id': post.id,
                                'title': post.title,
                                'selftext': post.selftext,
                                'content_to_code': f"{post.title}. {post.selftext}",
                                'author': str(post.author),
                                'score': post.score,
                                'num_comments': post.num_comments,
                                'created_utc': post.created_utc,
                                'created_date': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d'),
                                'subreddit': subreddit_name,
                                'url': post.url,
                                'permalink': f"https://reddit.com{post.permalink}",

                                # Query metadata
                                'query': query,
                                'query_layer': layer,
                                'query_subcategory': subcategory,
                                'period': period_name,
                                'content_type': 'post',
                                'parent_id': None  # Posts have no parent
                            }

                            collected_data.append(post_data)
                            posts_count += 1
                            period_posts += 1
                            stats[period_name][layer] += 1

                            # ==========================================
                            # COLLECT COMMENTS FROM THIS POST
                            # ==========================================
                            try:
                                # Get top-level comments
                                post.comments.replace_more(limit=0)  # Don't expand "more comments"
                                top_comments = post.comments.list()[:comments_per_post]

                                for comment in top_comments:
                                    # Check if comment is in time period
                                    if period_start <= comment.created_utc <= period_end:
                                        comment_data = {
                                            'post_id': comment.id,  # Comment ID
                                            'title': f"Comment on: {post.title}",
                                            'selftext': comment.body,
                                            'content_to_code': comment.body,
                                            'author': str(comment.author),
                                            'score': comment.score,
                                            'num_comments': 0,  # Comments don't have sub-comments counted
                                            'created_utc': comment.created_utc,
                                            'created_date': datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d'),
                                            'subreddit': subreddit_name,
                                            'url': f"https://reddit.com{comment.permalink}",
                                            'permalink': comment.permalink,

                                            # Query metadata
                                            'query': query,
                                            'query_layer': layer,
                                            'query_subcategory': subcategory,
                                            'period': period_name,
                                            'content_type': 'comment',
                                            'parent_id': post.id  # Link to parent post
                                        }

                                        collected_data.append(comment_data)
                                        comments_count += 1
                                        period_comments += 1
                                        stats[period_name][f"{layer}_comments"] += 1

                            except Exception as e:
                                # If comment collection fails, just skip
                                pass

                    print(f"{posts_count} posts, {comments_count} comments")

                    # Rate limiting
                    time.sleep(rate_limit_delay)

                except Exception as e:
                    print(f"Error: {str(e)[:50]}")
                    continue

            print(f"     Total for {period_name}: {period_posts} posts + {period_comments} comments")

    # Convert to DataFrame
    df = pd.DataFrame(collected_data)

    # Remove duplicates (same post/comment caught by multiple queries)
    if len(df) > 0:
        df = df.drop_duplicates(subset=['post_id'])

    print("\n" + "="*80)
    print("COLLECTION COMPLETE")
    print("="*80)
    print(f"\n Total items collected: {len(df)}")
    print(f" Posts: {len(df[df['content_type']=='post'])}")
    print(f" Comments: {len(df[df['content_type']=='comment'])}")
    print(f" Unique items: {df['post_id'].nunique()}")

    # Display statistics
    print("\n COLLECTION STATISTICS BY PERIOD & LAYER:")
    print("-"*80)

    for period in time_periods.keys():
        print(f"\n{period}:")
        period_df = df[df['period']==period]
        posts = len(period_df[period_df['content_type']=='post'])
        comments = len(period_df[period_df['content_type']=='comment'])
        print(f"  Posts: {posts} | Comments: {comments} | Total: {len(period_df)}")

    return df

# ============================================================================
# EXECUTE COLLECTION
# ============================================================================

print("\n Starting data collection (posts + comments)...")
print("  This will take several hours due to rate limiting")
print("  Progress will be saved periodically")

# Collect data
df_collected = collect_reddit_data(
    queries_list=ALL_QUERIES,
    subreddits=SUBREDDITS,
    time_periods=TIME_PERIODS,
    posts_per_query=20,      # Posts per query
    comments_per_post=10,      # Top comments per post
    rate_limit_delay=2        # 2 seconds between queries
)

# ============================================================================
# SAVE COLLECTED DATA (FIXED)
# ============================================================================

if len(df_collected) > 0:
    print("\n" + "="*80)
    print("SAVING COLLECTED DATA")
    print("="*80)

    # Function to clean text for Excel
    def clean_for_excel(text):
        """Remove illegal characters for Excel"""
        if pd.isna(text):
            return text

        import re
        # Remove control characters (except newline, tab, carriage return)
        text = str(text)
        text = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F-\x9F]', '', text)
        return text

    # Clean text columns for Excel
    print("\nðŸ§¹ Cleaning text for Excel compatibility...")

    text_columns = ['title', 'selftext', 'content_to_code']

    for col in text_columns:
        if col in df_collected.columns:
            df_collected[col] = df_collected[col].apply(clean_for_excel)

    print(" Text cleaned")

    # Save raw data
    print("\n Saving CSV...")
    df_collected.to_csv('reddit_extended_harm_queries_RAW.csv', index=False)
    print(" Saved: reddit_extended_harm_queries_RAW.csv")

    print("\n Saving Excel...")
    try:
        df_collected.to_excel('reddit_extended_harm_queries_RAW.xlsx', index=False)
        print(" Saved: reddit_extended_harm_queries_RAW.xlsx")
    except Exception as e:
        print(f"  Excel save failed: {str(e)[:100]}")
        print("   CSV file is complete and usable!")

    # Display summary
    print("\n" + "="*80)
    print("DATA SUMMARY")
    print("="*80)

    print(f"\nTotal items: {len(df_collected)}")
    print(f"  Posts: {len(df_collected[df_collected['content_type']=='post'])}")
    print(f"  Comments: {len(df_collected[df_collected['content_type']=='comment'])}")

    print(f"\n By period:")
    period_counts = df_collected['period'].value_counts().sort_index()
    for period, count in period_counts.items():
        pct = (count / len(df_collected)) * 100
        period_df = df_collected[df_collected['period']==period]
        posts = len(period_df[period_df['content_type']=='post'])
        comments = len(period_df[period_df['content_type']=='comment'])
        print(f"  {period:15s}: {count:4d} ({pct:5.1f}%) - {posts}p + {comments}c")

    print(f"\n  By layer:")
    layer_counts = df_collected['query_layer'].value_counts()
    for layer, count in layer_counts.items():
        pct = (count / len(df_collected)) * 100
        print(f"  {layer:35s}: {count:4d} posts ({pct:5.1f}%)")

    print(f"\n By subreddit:")
    sub_counts = df_collected['subreddit'].value_counts()
    for sub, count in sub_counts.items():
        pct = (count / len(df_collected)) * 100
        print(f"  r/{sub:20s}: {count:4d} ({pct:5.1f}%)")

    print(f"\n By content type:")
    type_counts = df_collected['content_type'].value_counts()
    for content_type, count in type_counts.items():
        pct = (count / len(df_collected)) * 100
        print(f"  {content_type:15s}: {count:4d} ({pct:5.1f}%)")

    # Layer Ã— Period crosstab
    print(f"\n LAYER Ã— PERIOD DISTRIBUTION:")
    print("-"*80)

    crosstab = pd.crosstab(df_collected['period'], df_collected['query_layer'])
    print(crosstab)

    # Save crosstab
    crosstab.to_csv('layer_period_distribution.csv')
    print("\n Saved: layer_period_distribution.csv")

else:
    print("\n  No data collected - check API credentials and queries")

print("\n" + "="*80)
print(" DATA COLLECTION SCRIPT COMPLETE")
print("="*80)